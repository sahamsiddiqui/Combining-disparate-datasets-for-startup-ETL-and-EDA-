# Combining Disparate Datasets for Startup (Normalization , Fuzzy Matching)
## Project Overview
**ReviewFriends** is a startup business that runs a review website with affiliate marketing. The challenge was to better understand user behavior by creating an analytical dataset from multiple data sources. These datasets come from both in-house systems and partner companies, and they are often **disparate**—meaning they don't share a common key for easy combination.

This project showcases how a data engineer can combine technical expertise with a deep understanding of business processes to solve this complex problem. The goal was to develop an **ETL (Extract, Transform, Load) pipeline** that merges these varied datasets into a single, meaningful **"one big table"** for business analysis.

The process involved not just technical skills like data cleaning, normalization, and fuzzy matching but also knowledge of the **business context** to align disparate data and create actionable insights.

### Key Steps:
1. **Understanding Business and Data**: First, it was crucial to understand the **business meaning** behind each dataset. This included identifying key data points, such as the **country of the user** (from IP addresses) and **timestamps** of user interactions with the affiliate UI. These elements were common across datasets but in different forms.

2. **Identifying Alignment**: Although the datasets didn’t have obvious keys to merge, there were business-specific dimensions that could be aligned. For instance, while the format of timestamps or user country data may differ, they represent the same business concepts. This understanding allowed us to create meaningful links across datasets.

3. **Normalization and Fuzzy Matching**: To make the data combinable, I applied **data normalization** (e.g., removing spaces and standardizing formats) and **fuzzy matching** (a technique that aligns similar but not identical values, like slightly different spellings of a country name).

4. **Creating a Composite Key**: Using the cleaned and aligned data, I generated a **composite key**—a unique identifier formed by combining relevant fields such as country and timestamp. This allowed us to merge the disparate datasets into one unified table.

5. **Automating the ETL Pipeline**: Finally, an automated & modular **Python ETL pipeline** was created to ingest, clean, normalize, and combine data from different sources into a unified analytical table, which could be used for downstream analysis.

6. **Generating Business Insights**: After the data was unified, an **Exploratory Data Analysis (EDA)** was conducted to extract valuable insights about user behavior. These insights were then turned into actionable recommendations for the business to optimize its growth strategy.

## Project Structure
The repository contains:
- **etl_reviewfriends.py**: A Python script that automates the end-to-end data processing pipeline, from data ingestion to transformation and loading.
- A Jupyter Notebook, **EDA_modified.ipynb**, that performs EDA on the unified dataset to derive business insights.
- **requirements.txt**: Contains all the libraries needed to run the script.
- **Raw data files**: Used for demonstration and testing.
- **onebigtable.csv**: The final unified dataset created through this pipeline, which can be used for analysis.

## Setup and Requirements
To run this project, you'll need **Python** installed along with the required libraries listed in `requirements.txt`.

To get started:
1. Clone the repository and navigate to the project folder.
2. Set up a virtual environment and activate it:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # For Linux/Mac
   .\venv\Scripts\activate   # For Windows
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Usage
1. **Run the ETL Pipeline**:
   - Ensure the raw data files are placed in the `data` folder.
   - Execute the pipeline script:
     ```bash
     python etl_reviewfriends.py
     ```
   - The output will be a unified dataset that can be saved locally or uploaded to a database for further analysis.

2. **Perform EDA**:
   - Install the IPython kernel:
     ```bash
     python -m ipykernel install --user --name=venv --display-name "Python (venv)"
     ```
   - Run Jupyter:
     ```bash
     jupyter notebook
     ```
   - Open the `EDA_modified.ipynb` Jupyter Notebook in the activated virtual environment.
   - Optional: Modify the file path to load the unified dataset generated by the pipeline, in case you do not want to use the provided *onebigtable.csv* unified dataset.
   - Run the notebook to explore the data and generate insights.

## Results and Insights
By combining both technical methods and business knowledge, this project successfully solves the problem of integrating disparate datasets into one unified analytical table.

### Key Deliverables:
- **Automated ETL Pipeline**:
  - Ingests raw data from various sources.
  - Applies business rules and technical methods (e.g., normalization and fuzzy matching) to align and combine data.
  - Loads the final dataset into a database or file for business analysis.

- **Exploratory Data Analysis**:
  - The unified data is then used to gain a deeper understanding of user behavior.
  - Insights from the data help inform strategic decisions, such as identifying high-value users and optimizing marketing efforts for business growth.

## Conclusion
This project demonstrates how a data engineer can leverage both **technical skills** and **business insights** to address complex data integration challenges. It highlights the importance of aligning technical processes with business goals to create valuable, actionable insights that drive growth.

Check out the detailed article on this at my Medium blog [here].  
For any question reach out to me through my linked 
